三、提出的方法
	本文提出了一种端到端的双路径特征学习框架，框架如图所示：
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
	该框架是由表征学习和度量学习两个阶段结合在一起的端到端网络，前者通过双路径网络来实现，共享部分全连接层，用于特征提取和特征嵌入



3 提议的方法

本文提出了VT-REID的双路径端到端特征学习框架，如图所示。该框架以端到端的方式学习特征表示和距离度量，同时保持高可辨性。 它包括两个主要组成部分：用于特征提取的双路径网络和用于特征学习的双向双约束最高级别丢失。 具体地，双路径网络利用部分共享结构通过同时建模特定模态和模态共享信息来学习多模态可共享特征。 双约束的最高等级损失确保所学习的特征表示具有足够的辨别力以区分不同的人与两个异质模态。 整合身份丢失以促进特征学习过程。

3.1双路径网络
我们提出了一种双路径网络来提取可见域和热域的特征。 具体而言，双路特征学习网络包含两部分：特征提取器和特征嵌入。 前特征提取器旨在捕获不同图像模态的模态特定信息。 后一特征嵌入侧重于学习多模态可共享空间，以弥合两种异质模态之间的差距。

特征提取器。 我们采用现成的图像特征提取器从两个异质模态中提取特征。 由于训练数据有限，采用ImageNet预训练的一般图像分类网络进行初始化，以增强训练过程，实现快速收敛。 请注意，热路径和可见路径在我们的跨模态人员重新识别任务中共享类似的网络结构。 主要原因是我们假设热图像的低级视觉图案（例如纹理，角落）与一般可见图像相似。 但是，两个流的参数分别进行优化，以捕获特定于模态的信息。

在我们的模型中，我们采用AlexNet作为可见和热路径的基线网络。 具体来说，我们采用预先训练的五个卷积层（conv1~conv5）和一个完全连接的层（大小为4096）作为初始化特征表示。 主要原因是浅层卷积层主要捕获可能在所有图像之间共享的低级视觉图案。 同时，我们在FC层之后添加另一个批量标准化层。

特征嵌入。 为了学习两个异质模态的判别嵌入空间，我们在双路径特征提取器的顶部引入了一个共享的完全连接层。 注意，共享完全连接层的权重以模拟模态共享信息。 如果没有，学习的可见和热图像特征可能位于完全不同的子空间中[Wang et al。，2016a; Wu等，2018a]。 以下部分中的实验结果表明，共享结构可以为VT-REID实现更好的性能，其中它充当投影函数以将两种不同的模态投影到公共空间中。 为了简化演示，我们将嵌入式功能与特征提取器一起表示为Fv（·）用于可见图像，而Ft（·）用于热图像。 给定可见图像Iv和热图像It，提取的特征（x和z）由x=Fv（Iv），z=Ft（It）表示。


3.2双重约束排名
在将可见图像和热图像转换为共享嵌入空间之后，我们提出了一种新颖的双向双约束顶级损失来指导特征学习目标。 学习目标主要包含交叉模态和模态内约束，如图5所示.首先，我们将重新审视一般排名损失。

排名损失重访。 给定一个小批量，它包含N可见图像和N热图像。 对于锚可见图像xi，其标签由yi表示，我们想要距离其正热图像zj应小于xi与负热图像zk之间的距离为a预定义边ρ1：D(xi, zj ) < D(xi, zk) − ρ1, ∀yi 6= yk, ∀yi = yj 。注意，所有输入特征向量x和z都是用于稳定收敛标准化的l2。 在我们提出的方法中，欧几里德距离被用作相似性测量，其中我们凭经验发现它实现了稍微好一些性能优于VT-REID的其他测量。 此外，我们进一步采用双向排名亏损策略限制跨模态的行人重新识别问题的整体学习。 双向排名亏损包含两种关系：可见到热量的三重态（一个锚可见图像，两个热图像）和热量到可见的三重态（一个锚定热图像，两个可见的图像）。双向排名损失由公式Lbi rank =X∀yi=yj,yi6=ykmax[ρ1 + D(xi, zj ) − D(xi, zk), 0]+X∀yi=yj ,yi6=ykmax[ρ1 + D(zi, xj ) − D(zi, xk), 0]制定，下标i和j代表相同的身份，而我和k是不同的身份。

跨模式排名最高的约束。 解决这个问题问题是大量的班级内距离可能是甚至比交叉模态变化引起的类间距离更大，我们采用了最高等级的约束[Hermans et al。，2017]来提高辨别力。 根本的想法是我们比较a的距离正可见热对和所有的最小距离相关的负可见 - 热对，而不是每个负对。排名靠前的损失限制的跨模态进一步发展为：Lcross =X∀yi= YJmax [ρ1+ D（xi，zj） - 分钟∀yi6= YKd（XI，zk），0]+X∀yi= YJmax [ρ1+ D（zi，xj） - 分钟∀yi6= YKd（ZI，xk），0]。双向交叉模态排名靠前的损失有两个主要优点：（1）排名靠前的约束确保了这一点最接近的交叉模态负样本远离最远的交叉模态正样本，因此有助于减少跨模态变化，同时保持高度可辨识性。 （2）双向培训策略确保学习的特征表示是模态不变的。 它证明了不同查询设置的稳健性（即可见如图所示，如热和热可见。4.3。

模态内排名最高的约束。 如上所述在第1节中，VT-REID也受到了课内的影响由于不同的姿势，观点导致的模态内变化为了解决这个问题，我们引入了另一种内部模态相似性约束来增强其稳健性学习特征表示到模态内变化。 上跨模式顶级损失的顶部，内部形态约束损失由Lintra =Xmax [ρ2 -  D（zj，zk），0]+Xmax [ρ2 -  D（xj，xk），0]计算，其中ρ2是预定义的边际。j和k代表与交叉模态中的排名相同的指数每个锚的迷你批次i。 这种内部形态排名靠前约束确保最难的交叉模态为负样品也应远离其相应的交叉模态阳性样品。 它保证了图像每种方式中的不同人也应该加以区别，特别是当交叉方式相等时。 2不适用于大规模培训。

整体嵌入损失。 由于上述排名损失会使特征学习过程与人们之间的潜在关系紧密相关，因此难以学习强大的特征表示通过简单地减少类内变化利用关系线索。 同时，可见和热图像特征可能存在于完全不同的特征节奏中，排名损失也可能陷入融合由于关系测量不正确导致的问题 因此，我们将身份信息整合到整体损失中功能。 为了在分类中的可行性和有效性，通过对每个进行处理来利用一般的softmax损失人的身份作为一个阶级。 以这种方式，身份具体信息被整合以增强稳健性。最终的损失函数是加权求和的三个组成部分，即双向交叉模态和内部模态排名靠前的约束和身份损失，由L = Lcross +λ1Lintra+λ2Lid定义，其中λ1和λ2是预定义的加权参数。

批量采样。 由于我们的双重约束排名一般人重新识别的损失略有不同任务，介绍小批量抽样的策略是必不可少的。 具体而言，N个人身份首先是随机的在每次迭代时选择，其中N是批量大小。 然后我们从两种不同的模态中随机选择一个可见图像和一个所选身份的热图像构建小批量，其中共有2 * N个图像进入网络进行培训。 以这种方式，在小批量，我们可以选择N锚可见图像来计算可见热顶级损失，并且N对应锚定热图像用于热可见顶级排名失利。 由于随机采样机制，将遍历所有可能的组件以获得全局最优。


最开始，我们认为随着深度的增加，网络效果不好，那是因为存在着梯度消失和梯度爆炸的原因。不过随着大家的努力，这些问题可以通过归一化初始化（即用特定的初始化算法）和归一化层（Batch Normailzation）来极大的缓解。

*****特征嵌入，将数据转换（降维）为固定大小的特征表示（矢量），以便于处理和计算（如求距离）。例如，针对用于说话者识别的语音信号训练的模型可以允许您将语音片段转换为数字向量，使得来自相同说话者的另一片段与原始向量具有小的距离（例如，欧几里德距离）。

*****embedding的主要目的是对（稀疏）特征进行降维，它降维的方式可以类比为一个全连接层（没有激活函数），通过 embedding 层的权重矩阵计算来降低维度。
